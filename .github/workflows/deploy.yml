name: Deploy

on:
  push:
    branches: [master]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Deployment environment'
        required: true
        default: 'production'
        type: choice
        options:
          - production
          - staging

env:
  NODE_VERSION: '20'
  # PNPM version auto-detected from package.json packageManager field
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  # ===========================================
  # JOB 1: DETECT CONFIGURATION
  # ===========================================
  detect-config:
    name: Detect Configuration
    runs-on: ubuntu-latest
    outputs:
      secrets_source: ${{ steps.detect.outputs.secrets_source }}
      ssm_param_name: ${{ steps.detect.outputs.ssm_param_name }}
      backup_enabled: ${{ steps.detect.outputs.backup_enabled }}
      rollback_enabled: ${{ steps.detect.outputs.rollback_enabled }}
      health_check_enabled: ${{ steps.detect.outputs.health_check_enabled }}
      deployment_mode: ${{ steps.detect.outputs.deployment_mode }}
      caddy_enabled: ${{ steps.detect.outputs.caddy_enabled }}
      caddy_mode: ${{ steps.detect.outputs.caddy_mode }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install yq
        run: |
          sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
          sudo chmod +x /usr/local/bin/yq

      - name: Detect configuration
        id: detect
        run: |
          CONFIG_FILE="config/project.config.yaml"

          # Read config values (use .example as fallback)
          if [ ! -f "$CONFIG_FILE" ]; then
            echo "Config file not found, using example"
            CONFIG_FILE="config/project.config.yaml.example"
          fi

          SECRETS_SOURCE=$(yq '.deployment.secrets.source // "auto"' $CONFIG_FILE)
          SSM_PARAM_PATH=$(yq '.deployment.secrets.ssmPath // ""' $CONFIG_FILE)
          # NOTE: REGISTRY_TYPE now comes from SSM/deployment.env, not project.config.yaml
          # This ensures SSM is the single source of truth for ALL runtime config
          BACKUP_ENABLED=$(yq '.deployment.backup.enabled // true' $CONFIG_FILE)
          ROLLBACK_ENABLED=$(yq '.deployment.rollback.enabled // true' $CONFIG_FILE)
          HEALTH_CHECK_ENABLED=$(yq '.deployment.healthCheck.enabled // true' $CONFIG_FILE)
          DEPLOYMENT_MODE=$(yq '.deployment.mode // "self-hosted-docker"' $CONFIG_FILE)
          CADDY_ENABLED=$(yq '.deployment.caddy.enabled // true' $CONFIG_FILE)
          CADDY_MODE=$(yq '.deployment.caddy.mode // "auto"' $CONFIG_FILE)

          # Determine SSM parameter name (prefer GitHub var, fallback to config)
          SSM_PARAM_NAME="${{ vars.SSM_PARAM_NAME }}"
          if [ -z "$SSM_PARAM_NAME" ]; then
            SSM_PARAM_NAME="$SSM_PARAM_PATH"
          fi

          # Auto-detect secrets source if set to "auto"
          if [ "$SECRETS_SOURCE" = "auto" ]; then
            if [ -n "$SSM_PARAM_NAME" ] && [ -n "${{ secrets.AWS_ACCESS_KEY_ID }}" ]; then
              echo "Auto-detected: AWS SSM Parameter Store"
              SECRETS_SOURCE="aws-ssm"
            elif [ -n "${{ secrets.POSTGRES_PASSWORD }}" ]; then
              echo "Auto-detected: GitHub Secrets"
              SECRETS_SOURCE="github-secrets"
            else
              echo "Auto-detected: Manual (.env on server)"
              SECRETS_SOURCE="manual"
            fi
          fi

          echo "secrets_source=$SECRETS_SOURCE" >> $GITHUB_OUTPUT
          echo "ssm_param_name=$SSM_PARAM_NAME" >> $GITHUB_OUTPUT
          echo "backup_enabled=$BACKUP_ENABLED" >> $GITHUB_OUTPUT
          echo "rollback_enabled=$ROLLBACK_ENABLED" >> $GITHUB_OUTPUT
          echo "health_check_enabled=$HEALTH_CHECK_ENABLED" >> $GITHUB_OUTPUT
          echo "deployment_mode=$DEPLOYMENT_MODE" >> $GITHUB_OUTPUT
          echo "caddy_enabled=$CADDY_ENABLED" >> $GITHUB_OUTPUT
          echo "caddy_mode=$CADDY_MODE" >> $GITHUB_OUTPUT

          echo "Configuration detected:"
          echo "  Secrets Source: $SECRETS_SOURCE"
          echo "  Backup Enabled: $BACKUP_ENABLED"
          echo "  Rollback Enabled: $ROLLBACK_ENABLED"
          echo "  Health Check Enabled: $HEALTH_CHECK_ENABLED"
          echo "  Deployment Mode: $DEPLOYMENT_MODE"
          echo "  Caddy Enabled: $CADDY_ENABLED"
          echo "  Caddy Mode: $CADDY_MODE"
          echo "  NOTE: Registry config comes from SSM/deployment.env (single source of truth)"

  # ===========================================
  # JOB 2: PREPARE ENV FROM AWS SSM
  # ===========================================
  prepare-env-ssm:
    name: Prepare Environment (AWS SSM)
    runs-on: ubuntu-latest
    needs: detect-config
    if: needs.detect-config.outputs.secrets_source == 'aws-ssm'

    steps:
      - name: Fetch from AWS SSM
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ vars.AWS_REGION }}
        run: |
          echo "Fetching secrets from SSM: ${{ needs.detect-config.outputs.ssm_param_name }}"
          aws ssm get-parameter \
            --name "${{ needs.detect-config.outputs.ssm_param_name }}" \
            --with-decryption \
            --query 'Parameter.Value' \
            --output text > deployment.env

          # Verify file exists and is not empty
          if [ ! -s deployment.env ]; then
            echo "Error: deployment.env is empty"
            exit 1
          fi

          echo "Successfully fetched $(wc -l < deployment.env) lines from SSM"

      - name: Upload env artifact
        uses: actions/upload-artifact@v4
        with:
          name: deployment-env
          path: deployment.env
          retention-days: 1

  # ===========================================
  # JOB 3: PREPARE ENV FROM GITHUB SECRETS
  # ===========================================
  prepare-env-github:
    name: Prepare Environment (GitHub Secrets)
    runs-on: ubuntu-latest
    needs: detect-config
    if: needs.detect-config.outputs.secrets_source == 'github-secrets'

    steps:
      - name: Build from GitHub Secrets
        run: |
          cat > deployment.env << 'ENV_FILE'
          # ===========================================
          # ATLASP2P PRODUCTION ENVIRONMENT
          # Generated from GitHub Secrets
          # ===========================================

          # Node Environment
          NODE_ENV=production

          # Database (Supabase Self-Hosted)
          POSTGRES_PASSWORD=${{ secrets.POSTGRES_PASSWORD }}
          POSTGRES_PORT=${{ secrets.POSTGRES_PORT || '5432' }}
          POSTGRES_DB=${{ secrets.POSTGRES_DB || 'postgres' }}

          # Supabase Configuration
          JWT_SECRET=${{ secrets.JWT_SECRET }}
          ANON_KEY=${{ secrets.ANON_KEY }}
          SERVICE_ROLE_KEY=${{ secrets.SERVICE_ROLE_KEY }}
          SUPABASE_SERVICE_ROLE_KEY=${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          NEXT_PUBLIC_SUPABASE_URL=${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON_KEY=${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}

          # API Configuration
          API_EXTERNAL_URL=${{ secrets.API_EXTERNAL_URL }}
          SUPABASE_URL=${{ secrets.SUPABASE_URL }}

          # SMTP Configuration (GoTrue)
          SMTP_ADMIN_EMAIL=${{ secrets.SMTP_ADMIN_EMAIL }}
          SMTP_HOST=${{ secrets.SMTP_HOST }}
          SMTP_PORT=${{ secrets.SMTP_PORT || '587' }}
          SMTP_USER=${{ secrets.SMTP_USER }}
          SMTP_PASS=${{ secrets.SMTP_PASS }}
          SMTP_SENDER_NAME=${{ secrets.SMTP_SENDER_NAME || 'AtlasP2P' }}

          # Email Provider (Application)
          RESEND_API_KEY=${{ secrets.RESEND_API_KEY }}
          SENDGRID_API_KEY=${{ secrets.SENDGRID_API_KEY }}

          # Cloudflare Turnstile
          TURNSTILE_SECRET_KEY=${{ secrets.TURNSTILE_SECRET_KEY }}
          NEXT_PUBLIC_TURNSTILE_SITE_KEY=${{ secrets.NEXT_PUBLIC_TURNSTILE_SITE_KEY }}

          # GeoIP
          MAXMIND_LICENSE_KEY=${{ secrets.MAXMIND_LICENSE_KEY }}

          # RPC Configuration
          RPC_HOST=${{ secrets.RPC_HOST }}
          RPC_PORT=${{ secrets.RPC_PORT }}
          RPC_USER=${{ secrets.RPC_USER }}
          RPC_PASS=${{ secrets.RPC_PASS }}

          # Admin Configuration
          ADMIN_EMAILS=${{ secrets.ADMIN_EMAILS }}

          # Dashboard Configuration
          DASHBOARD_USERNAME=${{ secrets.DASHBOARD_USERNAME || 'supabase' }}
          DASHBOARD_PASSWORD=${{ secrets.DASHBOARD_PASSWORD }}

          # Studio Configuration
          STUDIO_DEFAULT_ORGANIZATION=${{ secrets.STUDIO_DEFAULT_ORGANIZATION || 'Default Organization' }}
          STUDIO_DEFAULT_PROJECT=${{ secrets.STUDIO_DEFAULT_PROJECT || 'Default Project' }}

          # Analytics
          LOGFLARE_API_KEY=${{ secrets.LOGFLARE_API_KEY }}
          LOGFLARE_URL=${{ secrets.LOGFLARE_URL || 'http://analytics:4000' }}

          # Docker Configuration
          COMPOSE_PROJECT_NAME=${{ secrets.COMPOSE_PROJECT_NAME || 'atlasp2p' }}
          ENV_FILE

          # Verify file exists and is not empty
          if [ ! -s deployment.env ]; then
            echo "Error: deployment.env is empty"
            exit 1
          fi

          echo "Successfully created deployment.env with $(wc -l < deployment.env) lines"

      - name: Upload env artifact
        uses: actions/upload-artifact@v4
        with:
          name: deployment-env
          path: deployment.env
          retention-days: 1

  # ===========================================
  # JOB 4: BUILD AND PUSH IMAGES
  # ===========================================
  build-and-push:
    name: Build & Push Images
    runs-on: ubuntu-latest
    needs: [detect-config, prepare-env-ssm, prepare-env-github]
    if: always() && needs.detect-config.result == 'success'
    permissions:
      contents: read
      packages: write
      id-token: write  # For AWS ECR

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # Download env artifact from prepare-env jobs (if exists)
      - name: Download deployment.env artifact
        if: needs.detect-config.outputs.secrets_source != 'manual'
        uses: actions/download-artifact@v4
        with:
          name: deployment-env
          path: .

      - name: Verify deployment.env exists
        if: needs.detect-config.outputs.secrets_source != 'manual'
        run: |
          if [ ! -f deployment.env ]; then
            echo "ERROR: deployment.env not found in build context"
            exit 1
          fi
          echo "deployment.env exists with $(wc -l < deployment.env) lines"
          # Show which NEXT_PUBLIC vars are set (without values for security)
          echo "NEXT_PUBLIC_* variables found:"
          grep "^NEXT_PUBLIC_" deployment.env | cut -d'=' -f1 || true

      - name: Extract build-time environment variables
        id: build-env
        run: |
          if [ -f deployment.env ]; then
            echo "Using deployment.env from SSM/GitHub Secrets"
            # Source the file to get variables into current shell
            set -a
            source deployment.env
            set +a
          else
            echo "Using GitHub Secrets directly (manual mode)"
            NEXT_PUBLIC_SUPABASE_URL="${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}"
            NEXT_PUBLIC_SUPABASE_ANON_KEY="${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}"
            NEXT_PUBLIC_TURNSTILE_SITE_KEY="${{ secrets.NEXT_PUBLIC_TURNSTILE_SITE_KEY }}"
            NEXT_PUBLIC_ENABLE_TURNSTILE="${{ secrets.NEXT_PUBLIC_ENABLE_TURNSTILE }}"
            NEXT_PUBLIC_TURNSTILE_PROTECTED_ACTIONS="${{ secrets.NEXT_PUBLIC_TURNSTILE_PROTECTED_ACTIONS }}"
          fi

          # Output for next steps (Docker build-args)
          echo "NEXT_PUBLIC_SUPABASE_URL=$NEXT_PUBLIC_SUPABASE_URL" >> $GITHUB_OUTPUT
          echo "NEXT_PUBLIC_SUPABASE_ANON_KEY=$NEXT_PUBLIC_SUPABASE_ANON_KEY" >> $GITHUB_OUTPUT
          echo "NEXT_PUBLIC_TURNSTILE_SITE_KEY=$NEXT_PUBLIC_TURNSTILE_SITE_KEY" >> $GITHUB_OUTPUT
          echo "NEXT_PUBLIC_ENABLE_TURNSTILE=$NEXT_PUBLIC_ENABLE_TURNSTILE" >> $GITHUB_OUTPUT
          echo "NEXT_PUBLIC_TURNSTILE_PROTECTED_ACTIONS=$NEXT_PUBLIC_TURNSTILE_PROTECTED_ACTIONS" >> $GITHUB_OUTPUT

          # Debug: verify values are set (show only first 20 chars for security)
          echo "Extracted NEXT_PUBLIC_* variables:"
          echo "  SUPABASE_URL: ${NEXT_PUBLIC_SUPABASE_URL:0:40}..."
          echo "  ANON_KEY: ${NEXT_PUBLIC_SUPABASE_ANON_KEY:0:20}..."
          echo "  TURNSTILE_SITE_KEY: ${NEXT_PUBLIC_TURNSTILE_SITE_KEY:0:20}..."
          echo "  ENABLE_TURNSTILE: $NEXT_PUBLIC_ENABLE_TURNSTILE"

      # Extract registry configuration from SSM artifact (single source of truth)
      - name: Extract registry configuration
        id: registry-config
        run: |
          if [ -f deployment.env ]; then
            # Read registry config from SSM artifact
            REGISTRY_TYPE=$(grep "^REGISTRY_TYPE=" deployment.env | cut -d'=' -f2 || echo "ghcr")
            REGISTRY=$(grep "^REGISTRY=" deployment.env | cut -d'=' -f2 || echo "")
            IMAGE_PREFIX=$(grep "^IMAGE_PREFIX=" deployment.env | cut -d'=' -f2 || echo "")

            # For ECR, we need the full registry URL
            if [ "$REGISTRY_TYPE" = "ecr" ]; then
              ECR_REGISTRY=$(grep "^ECR_REGISTRY=" deployment.env | cut -d'=' -f2 || echo "$REGISTRY")
              AWS_REGION=$(grep "^AWS_REGION=" deployment.env | cut -d'=' -f2 || echo "us-west-2")
              echo "Registry: ECR ($ECR_REGISTRY)"
            else
              echo "Registry: GHCR"
            fi
          else
            # Manual mode - default to GHCR
            REGISTRY_TYPE="ghcr"
            REGISTRY="ghcr.io"
            IMAGE_PREFIX=""
            echo "Registry: GHCR (default for manual mode)"
          fi

          echo "registry_type=$REGISTRY_TYPE" >> $GITHUB_OUTPUT
          echo "registry=$REGISTRY" >> $GITHUB_OUTPUT
          echo "image_prefix=$IMAGE_PREFIX" >> $GITHUB_OUTPUT

          # Determine image base name (ECR uses IMAGE_PREFIX, GHCR uses github.repository)
          if [ "$REGISTRY_TYPE" = "ecr" ] && [ -n "$IMAGE_PREFIX" ]; then
            # ECR: use IMAGE_PREFIX from SSM (e.g., dingocoin/nodes-map-)
            echo "image_base=${IMAGE_PREFIX}" >> $GITHUB_OUTPUT
            echo "Using ECR image prefix: $IMAGE_PREFIX"
          else
            # GHCR: use github repository name (e.g., owner/repo-)
            REPO_NAME=$(echo "${{ github.repository }}" | tr '[:upper:]' '[:lower:]')
            echo "image_base=${REPO_NAME}-" >> $GITHUB_OUTPUT
            echo "Using GHCR image base: ${REPO_NAME}-"
          fi

          echo "Registry configuration extracted from SSM (single source of truth)"

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        # Version auto-detected from package.json packageManager field

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Build packages
        run: pnpm build
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ steps.build-env.outputs.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ steps.build-env.outputs.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
          NEXT_PUBLIC_TURNSTILE_SITE_KEY: ${{ steps.build-env.outputs.NEXT_PUBLIC_TURNSTILE_SITE_KEY }}
          NEXT_PUBLIC_ENABLE_TURNSTILE: ${{ steps.build-env.outputs.NEXT_PUBLIC_ENABLE_TURNSTILE }}
          NEXT_PUBLIC_TURNSTILE_PROTECTED_ACTIONS: ${{ steps.build-env.outputs.NEXT_PUBLIC_TURNSTILE_PROTECTED_ACTIONS }}

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # Conditional ECR authentication (registry_type from SSM artifact)
      - name: Configure AWS credentials (ECR)
        if: steps.registry-config.outputs.registry_type == 'ecr'
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ vars.AWS_REGION || 'us-east-1' }}

      - name: Login to Amazon ECR
        if: steps.registry-config.outputs.registry_type == 'ecr'
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      # Default GHCR authentication
      - name: Login to GitHub Container Registry
        if: steps.registry-config.outputs.registry_type == 'ghcr' || steps.registry-config.outputs.registry_type == ''
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      # Set registry URL based on type (from SSM artifact)
      - name: Set registry URL
        id: registry
        run: |
          if [ "${{ steps.registry-config.outputs.registry_type }}" = "ecr" ]; then
            echo "url=${{ steps.login-ecr.outputs.registry }}" >> $GITHUB_OUTPUT
          else
            echo "url=${{ env.REGISTRY }}" >> $GITHUB_OUTPUT
          fi

      - name: Extract metadata for web
        id: meta-web
        uses: docker/metadata-action@v5
        with:
          # Use image_base from SSM for ECR, or github.repository for GHCR
          images: ${{ steps.registry.outputs.url }}/${{ steps.registry-config.outputs.image_base }}web
          tags: |
            type=ref,event=branch
            type=sha,prefix=
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push web image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: Dockerfile.web
          push: true
          tags: ${{ steps.meta-web.outputs.tags }}
          labels: ${{ steps.meta-web.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            NEXT_PUBLIC_SUPABASE_URL=${{ steps.build-env.outputs.NEXT_PUBLIC_SUPABASE_URL }}
            NEXT_PUBLIC_SUPABASE_ANON_KEY=${{ steps.build-env.outputs.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
            NEXT_PUBLIC_TURNSTILE_SITE_KEY=${{ steps.build-env.outputs.NEXT_PUBLIC_TURNSTILE_SITE_KEY }}
            NEXT_PUBLIC_ENABLE_TURNSTILE=${{ steps.build-env.outputs.NEXT_PUBLIC_ENABLE_TURNSTILE }}
            NEXT_PUBLIC_TURNSTILE_PROTECTED_ACTIONS=${{ steps.build-env.outputs.NEXT_PUBLIC_TURNSTILE_PROTECTED_ACTIONS }}

      - name: Extract metadata for crawler
        id: meta-crawler
        uses: docker/metadata-action@v5
        with:
          # Use image_base from SSM for ECR, or github.repository for GHCR
          images: ${{ steps.registry.outputs.url }}/${{ steps.registry-config.outputs.image_base }}crawler
          tags: |
            type=ref,event=branch
            type=sha,prefix=
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push crawler image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: Dockerfile.crawler
          push: true
          tags: ${{ steps.meta-crawler.outputs.tags }}
          labels: ${{ steps.meta-crawler.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  # ===========================================
  # JOB 5: BACKUP DATABASE
  # ===========================================
  backup:
    name: Backup Database
    runs-on: ubuntu-latest
    needs: [detect-config, build-and-push]
    if: |
      needs.detect-config.outputs.backup_enabled == 'true' &&
      needs.detect-config.outputs.deployment_mode == 'self-hosted-docker' &&
      github.ref == 'refs/heads/master'
    environment: production

    steps:
      - name: Setup SSH
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/deploy_key
          chmod 600 ~/.ssh/deploy_key
          ssh-keyscan -H ${{ vars.SSH_HOST }} >> ~/.ssh/known_hosts

      - name: Create database backup
        run: |
          ssh -i ~/.ssh/deploy_key ${{ vars.DEPLOY_USER }}@${{ vars.SSH_HOST }} << 'BACKUP_SCRIPT'
            set -e
            cd ${{ vars.DEPLOY_PATH }}

            BACKUP_DIR="backups"
            mkdir -p "$BACKUP_DIR"

            # Create timestamped backup
            BACKUP_FILE="$BACKUP_DIR/backup-$(date +%Y%m%d-%H%M%S).sql"

            echo "Creating database backup: $BACKUP_FILE"
            docker compose -f docker-compose.yml -f docker-compose.prod.yml exec -T db pg_dump -U postgres > "$BACKUP_FILE"

            # Verify backup was created
            if [ ! -s "$BACKUP_FILE" ]; then
              echo "Error: Backup file is empty"
              exit 1
            fi

            # Keep only last 7 backups
            ls -t "$BACKUP_DIR"/backup-*.sql | tail -n +8 | xargs -r rm

            echo "Backup complete: $BACKUP_FILE ($(du -h "$BACKUP_FILE" | cut -f1))"
            echo "Total backups: $(ls -1 "$BACKUP_DIR"/backup-*.sql | wc -l)"
          BACKUP_SCRIPT

  # ===========================================
  # JOB 6: DEPLOY TO SERVER
  # ===========================================
  deploy:
    name: Deploy to Server
    runs-on: ubuntu-latest
    needs: [detect-config, prepare-env-ssm, prepare-env-github, build-and-push, backup]
    if: |
      always() &&
      github.ref == 'refs/heads/master' &&
      needs.build-and-push.result == 'success' &&
      (needs.backup.result == 'success' || needs.backup.result == 'skipped')
    environment: production

    steps:
      # Checkout code first (needed for Caddyfile template)
      # IMPORTANT: Must be first to avoid wiping downloaded artifacts
      - name: Checkout code (for Caddyfile)
        if: needs.detect-config.outputs.caddy_mode == 'host'
        uses: actions/checkout@v4

      - name: Setup SSH
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/deploy_key
          chmod 600 ~/.ssh/deploy_key
          ssh-keyscan -H ${{ vars.SSH_HOST }} >> ~/.ssh/known_hosts

      # Download env artifact if using SSM or GitHub Secrets
      - name: Download deployment.env artifact
        if: needs.detect-config.outputs.secrets_source != 'manual'
        uses: actions/download-artifact@v4
        with:
          name: deployment-env
          path: .

      # Copy .env to server if not manual
      - name: Copy .env to server
        if: needs.detect-config.outputs.secrets_source != 'manual'
        run: |
          scp -i ~/.ssh/deploy_key deployment.env ${{ vars.DEPLOY_USER }}@${{ vars.SSH_HOST }}:${{ vars.DEPLOY_PATH }}/.env
          echo "Copied deployment.env to server"

      - name: Deploy Caddyfile to host
        if: needs.detect-config.outputs.caddy_mode == 'host'
        run: |
          # Extract SITE_URL from deployment.env to use as domain
          DOMAIN=$(grep "^SITE_URL=" deployment.env | cut -d'=' -f2 | sed 's|https://||' | sed 's|http://||')
          if [ -z "$DOMAIN" ]; then
            echo "Error: SITE_URL not found in deployment.env"
            exit 1
          fi

          # Generate Caddyfile from template
          cat docker/Caddyfile.host.example | sed "s|nodes.example.com|$DOMAIN|g" > caddyfile-generated

          # Copy to server for comparison
          scp -i ~/.ssh/deploy_key caddyfile-generated ${{ vars.DEPLOY_USER }}@${{ vars.SSH_HOST }}:/tmp/caddyfile-new

          # Deploy and reload Caddy with diff checking
          ssh -i ~/.ssh/deploy_key ${{ vars.DEPLOY_USER }}@${{ vars.SSH_HOST }} << 'CADDY_SCRIPT'
            set -e
            # Determine site config name from project name
            PROJECT_NAME=$(basename ${{ vars.DEPLOY_PATH }})
            CADDY_SITE="/etc/caddy/sites/${PROJECT_NAME}.Caddyfile"

            echo "Target Caddyfile: $CADDY_SITE"

            # Check if Caddyfile exists and compare
            if [ -f "$CADDY_SITE" ] && sudo diff -q "$CADDY_SITE" /tmp/caddyfile-new > /dev/null 2>&1; then
              # Files are identical - skip deployment
              echo "✓ Caddyfile unchanged - skipping deployment"
              rm /tmp/caddyfile-new
            else
              # Files differ or no existing file - deploy
              if [ -f "$CADDY_SITE" ]; then
                echo "Caddyfile has changed, deploying update..."
                # Backup existing config
                sudo cp "$CADDY_SITE" "${CADDY_SITE}.backup-$(date +%Y%m%d-%H%M%S)"
                echo "✓ Backed up existing Caddyfile"
              else
                echo "No existing Caddyfile found, deploying new config..."
              fi

              # Deploy new config
              sudo mv /tmp/caddyfile-new "$CADDY_SITE"
              sudo chown root:root "$CADDY_SITE"
              sudo chmod 644 "$CADDY_SITE"
              echo "✓ Deployed new Caddyfile"

              # Validate before reloading
              if sudo caddy validate --config /etc/caddy/Caddyfile; then
                echo "✓ Caddyfile validation passed"
                sudo systemctl reload caddy
                echo "✓ Caddy reloaded successfully"
              else
                echo "✗ Caddyfile validation failed"
                # Restore backup if it exists
                LATEST_BACKUP=$(ls -t "${CADDY_SITE}.backup-"* 2>/dev/null | head -1)
                if [ -n "$LATEST_BACKUP" ]; then
                  sudo cp "$LATEST_BACKUP" "$CADDY_SITE"
                  echo "✓ Restored backup: $LATEST_BACKUP"
                fi
                exit 1
              fi
            fi
          CADDY_SCRIPT

          echo "Caddyfile deployment complete"

      - name: Deploy application
        run: |
          ssh -i ~/.ssh/deploy_key ${{ vars.DEPLOY_USER }}@${{ vars.SSH_HOST }} bash -s << 'DEPLOY_SCRIPT'
            set -e
            cd ${{ vars.DEPLOY_PATH }}

            # Set deployment mode
            DEPLOYMENT_MODE="${{ needs.detect-config.outputs.deployment_mode }}"

            echo "Loading environment configuration..."
            if [ -f .env ]; then
              set -a  # Export all variables for subprocesses (aws cli, docker)
              source .env
              set +a
              echo "Environment loaded successfully"
            else
              echo "Warning: .env not found (manual mode)"
            fi

            # Determine compose files based on deployment mode
            # Note: prod files are OVERRIDES that require the base docker-compose.yml
            DEPLOYMENT_MODE="${DEPLOYMENT_MODE:-self-hosted-docker}"
            if [ "$DEPLOYMENT_MODE" = "self-hosted-cloud" ]; then
              COMPOSE_FILES="-f docker-compose.yml -f docker-compose.cloud-prod.yml"
            else
              COMPOSE_FILES="-f docker-compose.yml -f docker-compose.prod.yml"
            fi
            echo "Using compose files: $COMPOSE_FILES"

            # Authenticate to registry
            if [ "$REGISTRY_TYPE" = "ecr" ]; then
              echo "Authenticating to AWS ECR..."
              AWS_REGION="${AWS_REGION:-us-east-1}"
              aws ecr get-login-password --region "$AWS_REGION" | \
                docker login --username AWS --password-stdin "$ECR_REGISTRY"
              echo "ECR authentication successful"
            else
              echo "Using GitHub Container Registry (public images, no auth needed)"
            fi

            # Pull latest images
            echo "Pulling latest images..."
            docker compose $COMPOSE_FILES pull

            # Run database migrations using Supabase CLI
            echo "Running database migrations..."
            if [ -d supabase/migrations ]; then
              # Install Supabase CLI if not present
              if ! command -v supabase &> /dev/null; then
                echo "Installing Supabase CLI..."
                curl -fsSL https://github.com/supabase/cli/releases/latest/download/supabase_linux_amd64.tar.gz | tar -xz
                sudo mv supabase /usr/local/bin/
              fi

              # Link to Supabase project and push migrations
              if [ "$DEPLOYMENT_MODE" = "self-hosted-cloud" ]; then
                # Cloud mode: link to Supabase Cloud project
                echo "Linking to Supabase Cloud project..."
                if [ -n "$SUPABASE_PROJECT_REF" ] && [ -n "$SUPABASE_ACCESS_TOKEN" ]; then
                  supabase link --project-ref "$SUPABASE_PROJECT_REF" --password "$POSTGRES_PASSWORD"
                  echo "Pushing migrations to Supabase Cloud..."
                  supabase db push
                else
                  echo "Warning: SUPABASE_PROJECT_REF or SUPABASE_ACCESS_TOKEN not set, skipping migrations"
                fi
              else
                # Docker mode: link to local Supabase instance
                echo "Linking to local Supabase instance..."
                docker compose $COMPOSE_FILES up -d db
                sleep 5

                # Get database connection details from .env
                DB_HOST="${POSTGRES_HOST:-localhost}"
                DB_PORT="${POSTGRES_PORT:-5432}"
                DB_USER="${POSTGRES_USER:-postgres}"
                DB_NAME="${POSTGRES_DB:-postgres}"

                supabase db push --db-url "postgresql://$DB_USER:$POSTGRES_PASSWORD@$DB_HOST:$DB_PORT/$DB_NAME"
              fi
              echo "Migrations complete"
            else
              echo "No migrations directory found, skipping"
            fi

            # Start services
            echo "Starting services..."
            docker compose $COMPOSE_FILES up -d --remove-orphans

            # Wait for containers to start
            sleep 5

            # Verify containers are running
            echo "Verifying containers..."
            docker compose $COMPOSE_FILES ps

            # Cleanup old images
            echo "Cleaning up old images..."
            docker image prune -f

            echo "Deployment complete!"
          DEPLOY_SCRIPT

  # ===========================================
  # JOB 7: HEALTH CHECK
  # ===========================================
  health-check:
    name: Health Check
    runs-on: ubuntu-latest
    needs: [detect-config, deploy]
    if: |
      always() &&
      needs.detect-config.outputs.health_check_enabled == 'true' &&
      needs.deploy.result == 'success'
    environment: production

    steps:
      - name: Setup SSH
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/deploy_key
          chmod 600 ~/.ssh/deploy_key
          ssh-keyscan -H ${{ vars.SSH_HOST }} >> ~/.ssh/known_hosts

      - name: Run health checks
        env:
          DEPLOY_PATH: ${{ vars.DEPLOY_PATH }}
        run: |
          # Run health check via SSH with explicit exit code capture
          ssh -i ~/.ssh/deploy_key ${{ vars.DEPLOY_USER }}@${{ vars.SSH_HOST }} "cd $DEPLOY_PATH && bash" << 'HEALTH_SCRIPT'

            # Load health check config from .env (ignore errors in sourcing)
            if [ -f .env ]; then
              source .env 2>/dev/null || true
            fi

            ENDPOINT="${HEALTH_CHECK_ENDPOINT:-/api/stats}"
            TIMEOUT="${HEALTH_CHECK_TIMEOUT:-30}"
            RETRIES="${HEALTH_CHECK_RETRIES:-3}"
            PORT="${HEALTH_CHECK_PORT:-4000}"

            echo "Health check configuration:"
            echo "  Endpoint: $ENDPOINT"
            echo "  Timeout: ${TIMEOUT}s"
            echo "  Retries: $RETRIES"
            echo "  Port: $PORT"

            # Retry loop with explicit status tracking
            check_passed=0
            for i in $(seq 1 $RETRIES); do
              echo ""
              echo "Attempt $i/$RETRIES..."

              if curl -f -s --max-time "$TIMEOUT" "http://localhost:${PORT}${ENDPOINT}" > /dev/null 2>&1; then
                echo "Health check PASSED!"
                check_passed=1
                break
              fi

              echo "Health check failed"
              if [ "$i" -lt "$RETRIES" ]; then
                echo "Waiting 10 seconds before retry..."
                sleep 10
              fi
            done

            if [ "$check_passed" -eq 1 ]; then
              exit 0
            else
              echo ""
              echo "Health check FAILED after $RETRIES attempts"
              exit 1
            fi
          HEALTH_SCRIPT

  # ===========================================
  # JOB 8: ROLLBACK ON FAILURE
  # ===========================================
  rollback:
    name: Rollback Deployment
    runs-on: ubuntu-latest
    needs: [detect-config, backup, deploy, health-check]
    if: |
      always() &&
      needs.detect-config.outputs.rollback_enabled == 'true' &&
      (needs.deploy.result == 'failure' || needs.health-check.result == 'failure')
    environment: production

    steps:
      - name: Setup SSH
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/deploy_key
          chmod 600 ~/.ssh/deploy_key
          ssh-keyscan -H ${{ vars.SSH_HOST }} >> ~/.ssh/known_hosts

      - name: Rollback to previous state
        run: |
          ssh -i ~/.ssh/deploy_key ${{ vars.DEPLOY_USER }}@${{ vars.SSH_HOST }} << 'ROLLBACK_SCRIPT'
            set -e
            cd ${{ vars.DEPLOY_PATH }}

            echo "====================================="
            echo "DEPLOYMENT FAILED - INITIATING ROLLBACK"
            echo "====================================="

            # Find latest backup
            LATEST_BACKUP=$(ls -t backups/backup-*.sql 2>/dev/null | head -n 1)

            if [ -z "$LATEST_BACKUP" ]; then
              echo "ERROR: No backup found - cannot rollback database"
              echo "Containers will be restarted but database state is unknown"
            else
              echo "Restoring database from: $LATEST_BACKUP"

              # Stop containers to prevent writes during restore
              docker compose -f docker-compose.yml -f docker-compose.prod.yml stop web crawler

              # Restore database
              docker compose -f docker-compose.yml -f docker-compose.prod.yml exec -T db psql -U postgres < "$LATEST_BACKUP"

              echo "Database restored successfully"
            fi

            # Restart all containers (will use previous images since pull failed/was not done)
            echo "Restarting containers..."
            docker compose -f docker-compose.yml -f docker-compose.prod.yml restart

            # Wait for containers
            sleep 5

            # Verify containers are running
            echo "Container status:"
            docker compose -f docker-compose.yml -f docker-compose.prod.yml ps

            echo "====================================="
            echo "ROLLBACK COMPLETE"
            echo "====================================="
          ROLLBACK_SCRIPT

      - name: Notify rollback
        if: always()
        run: |
          echo "::error::Deployment failed and was rolled back"
          echo "Check the logs above for details on what went wrong"
